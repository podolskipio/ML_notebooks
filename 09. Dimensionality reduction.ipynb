{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Many problems in machine learning involve thousands or more features for each training instance and it can make the training process extremely slow. This problem is called _the curse of dimensionality_.\n",
    "\n",
    "However, it is possible to reduce the number of features. It not only allows us to speed up training but also it is usable for data visualization.(reducing number of features down to two)\n",
    "\n",
    "There are two main approaches to dimensionality reduction: projection and Manifold Learning.\n",
    "\n",
    "## Projection\n",
    "\n",
    "Some features are almost constant, while others are highly correlated. As a result, training instances lie close to some lower dimensional subspace of the high dimensional space. \n",
    "\n",
    "Here all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space.\n",
    "\n",
    "![source](img/subspace_PCA.png)\n",
    "Source: Hands on Machine Learning with Scikit-Learn and Keras\n",
    "\n",
    "We project every training instance onto this subspace and we get the new 2D dataset.\n",
    "\n",
    "![source](img/projection.png)\n",
    "Source: Hands on Machine Learning with Scikit-Learn and Keras\n",
    "\n",
    "The axes correspond to new features z1 and z2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA in Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm.\n",
    "\n",
    "It identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "In order to project the training set onto a lower-dimensional hyperplane, we need to choose the right hyperplane that preserves the maximum variance.(to lose less information than the other projections)\n",
    "\n",
    "Example of selecting the subspace onto which to project:\n",
    "\n",
    "![source](img/selecting_subspace.png)\n",
    "Source: Hands on Machine Learning with Scikit-Learn and Keras\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the training set. The unit vector that defines the $i^{th}$ axis is called the $i^{th}$ principal component (PC).\n",
    "\n",
    "To find the principal components of a training set, there is a standard matrix factorization technique called Singular Value Decomposition (SVD). It can decompose the training set matrix X into the dot product of three matrices $U \\cdot \\sum \\cdot V^T$, where $V^T$ contains all the principal components that we are looking for.\n",
    "\n",
    "After identifying all the principal components, we can start reducing the dimensionality to _d_ dimensions by projecting it onto the hyperplane defined by the first _d_ principal components. \n",
    "It is done by simply by computing the dot product of the training set matrix __X__ by the matrix $\\textbf{W}_d$(matrix containing the first d principal components).\n",
    "\n",
    "\\begin{equation*}\n",
    "X_{d-proj} = X \\cdot W_d\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very useful information is the explained variance ratio of each principal component, this is the proportion of the dataset’s variance that lies along the axis of each principal component. (Here it says that  84.2% of the dataset’s variance lies along the first axis, and 14.6% lies along the second axis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will recover 3d points projected on the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3D_inv = pca.inverse_transform(X2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PCA multiple times on slightly different datasets may result in different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26203346,  0.42067648],\n",
       "       [-0.08001485, -0.35272239],\n",
       "       [ 1.17545763,  0.36085729],\n",
       "       [ 0.89305601, -0.30862856],\n",
       "       [ 0.73016287, -0.25404049]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010170337792848549"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(np.square(X3D_inv - X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA object gives access to the principal components that it computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.93636116, -0.29854881, -0.18465208],\n",
       "       [ 0.34027485, -0.90119108, -0.2684542 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "\n",
    "Is is a good practice to choose the number of dimensions that add up to a sufficiently large portion of the variance. \n",
    "\n",
    "The code below computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) \n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be also done easier with scikit by setting n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95) \n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But projection is not always the best approach to dimensionality reduction, example below:\n",
    "\n",
    "![source](img/swiss_roll.png)\n",
    "Source: Hands on Machine Learning with Scikit-Learn and Keras\n",
    "\n",
    "\n",
    "Projecting onto a plane would squash different layers of the Swiss roll together.\n",
    "\n",
    "![source](img/projected_swiss_roll.png)\n",
    "Source: Hands on Machine Learning with Scikit-Learn and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Swiss roll is an example of a 2D manifold, which is a 2D shape that can be bent and twisted in a higher-dimensional space. \n",
    "\n",
    "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie, this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. \n",
    "\n",
    "The manifold assumption is often accompanied by another implicit assumption: that the task will be simpler if expressed in the lower-dimensional space of the manifold.\n",
    "\n",
    "![source](img/manifold.png)\n",
    "Source: Hands on Machine Learning with Scikit-Learn and Keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
